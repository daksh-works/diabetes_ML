{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pima Indians Diabetes ML\n",
    "Predicts the onset of diabetes based on diagnostic measures\n",
    "\n",
    "https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data=np.loadtxt(r\"C:\\Users\\Daksh Gohil\\Desktop\\Daksh\\Data Sets\\diabetes.csv\", delimiter=',')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=data[0:510,0:-1].T\n",
    "Y_train=data[0:510,-1:].T\n",
    "\n",
    "X_test=data[510:,0:-1].T\n",
    "Y_test=data[510:,-1:].T\n",
    "# print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I propose two hidden layers, both of them with tanh() as the activation function. The first hidden layer will have 8 Neurons and the second hidden layer will have 4 neurons. Finally the output will either contain 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09228464 0.09656339 0.01501579 0.07344358 0.03350347 0.07365073\n",
      "  0.07085321 0.0088028 ]\n",
      " [0.03577437 0.06227992 0.00363057 0.09394349 0.03787341 0.03913413\n",
      "  0.06444834 0.0831743 ]\n",
      " [0.02962919 0.07998721 0.05616443 0.08882943 0.0185011  0.08134139\n",
      "  0.01909659 0.03373308]\n",
      " [0.05879379 0.03460653 0.07561735 0.07633309 0.09848976 0.09423226\n",
      "  0.03212581 0.08725587]\n",
      " [0.07392461 0.0441785  0.02256847 0.01526495 0.09462057 0.00468283\n",
      "  0.01236916 0.04826984]\n",
      " [0.06923404 0.02448317 0.05557802 0.0510792  0.01017547 0.04448155\n",
      "  0.05474736 0.08094236]\n",
      " [0.00450038 0.0086333  0.00206127 0.02835618 0.00469282 0.0446063\n",
      "  0.04205599 0.07800057]\n",
      " [0.0463817  0.01307975 0.07194722 0.00388598 0.03293028 0.06014618\n",
      "  0.06526846 0.06006888]]\n",
      "\n",
      "[[0.00610751 0.08181857 0.03725797 0.07121775 0.08801767 0.00289812\n",
      "  0.05991212 0.00207139]\n",
      " [0.00734903 0.0564099  0.08130369 0.01874391 0.05824125 0.01211508\n",
      "  0.00775791 0.06282456]\n",
      " [0.04315799 0.07407784 0.04792316 0.04047227 0.0029006  0.07092444\n",
      "  0.08714795 0.01733647]\n",
      " [0.06853928 0.06768928 0.00469218 0.01398571 0.00354035 0.06023415\n",
      "  0.07250008 0.02707247]]\n",
      "\n",
      "[[0.08811147 0.02734991 0.03982384 0.03804719]]\n"
     ]
    }
   ],
   "source": [
    "#Initialise weights and bases to random values here!\n",
    "b1=np.zeros((8,1))\n",
    "b2=np.zeros((4,1))\n",
    "b3=np.zeros((1,1))\n",
    "W1=np.random.rand(8,8) *0.1\n",
    "W2=np.random.rand(4,8) *0.1\n",
    "W3=np.random.rand(1,4) *0.1\n",
    "print(W1,W2,W3,sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is the main code which will iterate through the neural network and generate output. It also contains code to train the neural network and check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # TRAINING THE NEURAL NETWORK\\nfor epoch in range(1000):\\n    #FORWARD PROPAGATION\\n    X1=W1 @ X_train + b1\\n    A1=np.tanh(X1)\\n\\n    X2=W2 @ A1 + b2\\n    A2=np.tanh(X2)\\n\\n    X3=W3 @ A2 +b3\\n    Y_train_calculated=np.tanh(X3)\\n    Y_train_threshold = (Y_train_calculated >= 0.5).astype(int)\\n\\n    #BACKWARD PROPAGATION\\n    dW3=(((Y_train_calculated-Y_train)*(1-Y_train_calculated**2)) @ X2.T)/510\\n    db3=np.sum((Y_train_calculated-Y_train)*(1-Y_train_calculated**2),axis=1,keepdims=True)/510\\n\\n    dW2 = (((W3.T @ ((Y_train_calculated - Y_train) * (1 - Y_train_calculated**2))) * (1 - A2**2)) @ A1.T) / 510\\n    db2 = np.sum((W3.T @ (Y_train_calculated - Y_train) * (1 - Y_train_calculated**2)) * (1 - A2**2), axis=1, keepdims=True) / A1.shape[1]\\n\\n    dW1 = (((W2.T @ ((W3.T @ ((Y_train_calculated - Y_train) * (1 - Y_train_calculated**2))) * (1 - A2**2))) * (1 - A1**2)) @ X_train.T) / 510  # (8, 8)\\n    db1 = np.sum(((W2.T @ ((W3.T @ ((Y_train_calculated - Y_train) * (1 - Y_train_calculated**2))) * (1 - A2**2))) * (1 - A1**2)), axis=1, keepdims=True) / X_train.shape[1]  # (8, 1)\\n    # print(\\'1st time\\',dW1)\\n\\n    # dW1=((1-A1**2)*(W2.T @ ((1-A2**2)*(W3.T@(2*(Y_train_calculated-Y_train)*(1-Y_train_calculated**2))))))@X_train.T / 510\\n    # print(\"\\n2nd time\",dW1)\\n\\n    # GRADIENT DESCENT\\n    W3=W3-0.1*dW3\\n    W2=W2-0.1*dW2\\n    W1=W1-0.1*dW1\\n    b3=b3-0.1*db3\\n    b2=b2-0.1*db2\\n    b1=b1-0.1*db1 '"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAINING THE NEURAL NETWORK\n",
    "for epoch in range(1000):\n",
    "    #FORWARD PROPAGATION\n",
    "    X1=W1 @ X_train + b1\n",
    "    A1=np.tanh(X1)\n",
    "\n",
    "    X2=W2 @ A1 + b2\n",
    "    A2=np.tanh(X2)\n",
    "\n",
    "    X3=W3 @ A2 +b3\n",
    "    Y_train_calculated=np.tanh(X3)\n",
    "    Y_train_threshold = (Y_train_calculated >= 0.5).astype(int)\n",
    "\n",
    "    #BACKWARD PROPAGATION\n",
    "    dW3=(((Y_train_calculated-Y_train)*(1-Y_train_calculated**2)) @ X2.T)/510\n",
    "    db3=np.sum((Y_train_calculated-Y_train)*(1-Y_train_calculated**2),axis=1,keepdims=True)/510\n",
    "\n",
    "    dW2 = (((W3.T @ ((Y_train_calculated - Y_train) * (1 - Y_train_calculated**2))) * (1 - A2**2)) @ A1.T) / 510\n",
    "    db2 = np.sum((W3.T @ (Y_train_calculated - Y_train) * (1 - Y_train_calculated**2)) * (1 - A2**2), axis=1, keepdims=True) / A1.shape[1]\n",
    "\n",
    "    dW1 = (((W2.T @ ((W3.T @ ((Y_train_calculated - Y_train) * (1 - Y_train_calculated**2))) * (1 - A2**2))) * (1 - A1**2)) @ X_train.T) / 510  # (8, 8)\n",
    "    db1 = np.sum(((W2.T @ ((W3.T @ ((Y_train_calculated - Y_train) * (1 - Y_train_calculated**2))) * (1 - A2**2))) * (1 - A1**2)), axis=1, keepdims=True) / X_train.shape[1]  # (8, 1)\n",
    "    # print('1st time',dW1)\n",
    "\n",
    "    # dW1=((1-A1**2)*(W2.T @ ((1-A2**2)*(W3.T@(2*(Y_train_calculated-Y_train)*(1-Y_train_calculated**2))))))@X_train.T / 510\n",
    "    # print(\"\\n2nd time\",dW1)\n",
    "\n",
    "    # GRADIENT DESCENT\n",
    "    W3=W3-0.1*dW3\n",
    "    W2=W2-0.1*dW2\n",
    "    W1=W1-0.1*dW1\n",
    "    b3=b3-0.1*db3\n",
    "    b2=b2-0.1*db2\n",
    "    b1=b1-0.1*db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.44186046511628%\n"
     ]
    }
   ],
   "source": [
    "# TESTING THE NEURAL NETWORK\n",
    "# Forward Propagation for Test Data\n",
    "X1_test = W1 @ X_test + b1\n",
    "A1_test = np.tanh(X1_test)\n",
    "\n",
    "X2_test = W2 @ A1_test + b2\n",
    "A2_test = np.tanh(X2_test)\n",
    "\n",
    "X3_test = W3 @ A2_test + b3\n",
    "Y_test_calculated = np.tanh(X3_test)\n",
    "\n",
    "# Convert the continuous output to binary using threshold 0.5\n",
    "Y_test_threshold = (Y_test_calculated >= 0.5).astype(int)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = np.mean(Y_test_threshold == Y_test) * 100\n",
    "print(f\"Accuracy: {accuracy}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
